{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7230688,"sourceType":"datasetVersion","datasetId":4186574},{"sourceId":5297,"sourceType":"modelInstanceVersion","modelInstanceId":4079},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"! pip install langchain llamaapi langchain_experimental openai faiss-cpu tiktoken awadb -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llamaapi import LlamaAPI\nfrom langchain_experimental.llms import ChatLlamaAPI\nfrom langchain.chains import LLMChain\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.embeddings import AlephAlphaAsymmetricSemanticEmbedding,AlephAlphaSymmetricSemanticEmbedding,LlamaCppEmbeddings\nfrom langchain.embeddings.awa import AwaEmbeddings\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:00:48.792670Z","iopub.execute_input":"2023-12-18T22:00:48.793706Z","iopub.status.idle":"2023-12-18T22:00:48.993599Z","shell.execute_reply.started":"2023-12-18T22:00:48.793668Z","shell.execute_reply":"2023-12-18T22:00:48.991100Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/books-ds/blueprints-for-text-analytics-using-python-machine-learning-based-solutions-for-common-real-world-nlp-applications-149207408x-9781492074083_compress.pdf'\nloader = PyPDFLoader(PATH)\nsplitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=100)\ndata =  loader.load_and_split(splitter)\ndata[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:00:49.793464Z","iopub.execute_input":"2023-12-18T22:00:49.794111Z","iopub.status.idle":"2023-12-18T22:00:58.932457Z","shell.execute_reply.started":"2023-12-18T22:00:49.794075Z","shell.execute_reply":"2023-12-18T22:00:58.931517Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Document(page_content='Jens Albrecht,  \\nSidharth Ramachandran  \\n& Christian Winkler\\nBlueprints  \\n for Text Analytics \\nUsing Python\\nMachine Learning-Based Solutions for  \\nCommon Real World (NLP) Applications', metadata={'source': '/kaggle/input/books-ds/blueprints-for-text-analytics-using-python-machine-learning-based-solutions-for-common-real-world-nlp-applications-149207408x-9781492074083_compress.pdf', 'page': 0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we have load and split our data into chunks , then we have to embed them to store in vector database","metadata":{}},{"cell_type":"code","source":"! pip install openai==0.28.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['OPENAI_API_KEY'] = 'sk-'\nquery = \"What is the content of the document?\"\nembeddings = OpenAIEmbeddings(show_progress_bar=True)\nv1 = embeddings.embed_query(query)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:00:58.934116Z","iopub.execute_input":"2023-12-18T22:00:58.934495Z","iopub.status.idle":"2023-12-18T22:00:59.561213Z","shell.execute_reply.started":"2023-12-18T22:00:58.934466Z","shell.execute_reply":"2023-12-18T22:00:59.560262Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb6732606f7d4391bd6bbe9085039454"}},"metadata":{}}]},{"cell_type":"code","source":"len(v1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom numpy.linalg import norm\n\ndef cosine_similarity(vector1:np.array,vector2:np.array):\n    return (vector1@vector2)/(norm(vector1)*norm(vector2))\nvector1 = np.array(embeddings.embed_query('AI'))\nvector2 = np.array(embeddings.embed_query('Deep learning'))\nprint(cosine_similarity(vector1,vector2))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:00:59.562664Z","iopub.execute_input":"2023-12-18T22:00:59.563095Z","iopub.status.idle":"2023-12-18T22:01:05.403072Z","shell.execute_reply.started":"2023-12-18T22:00:59.563057Z","shell.execute_reply":"2023-12-18T22:01:05.402130Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9368cfc1792e443ea7397531a8612122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba93949e5fea4a6ca2462bba47767b25"}},"metadata":{}},{"name":"stdout","text":"0.8197981424155507\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install cohere","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.embeddings import CohereEmbeddings\nos.environ['COHERE_API_KEY'] = ''\nembeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:01:05.404761Z","iopub.execute_input":"2023-12-18T22:01:05.405056Z","iopub.status.idle":"2023-12-18T22:01:05.505485Z","shell.execute_reply.started":"2023-12-18T22:01:05.405031Z","shell.execute_reply":"2023-12-18T22:01:05.504608Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"len(embeddings.embed_query('AI'))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:01:09.586213Z","iopub.execute_input":"2023-12-18T22:01:09.586707Z","iopub.status.idle":"2023-12-18T22:01:09.808906Z","shell.execute_reply.started":"2023-12-18T22:01:09.586662Z","shell.execute_reply":"2023-12-18T22:01:09.807898Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"384"},"metadata":{}}]},{"cell_type":"code","source":"# create index in the vector database FAISS is local database similer to sqlite\nindex = FAISS.from_documents(data,embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:01:10.513652Z","iopub.execute_input":"2023-12-18T22:01:10.514054Z","iopub.status.idle":"2023-12-18T22:01:12.109247Z","shell.execute_reply.started":"2023-12-18T22:01:10.514023Z","shell.execute_reply":"2023-12-18T22:01:12.108332Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"index.similarity_search_with_relevance_scores(\n    \"NLP Pipeline\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:01:15.195536Z","iopub.execute_input":"2023-12-18T22:01:15.196284Z","iopub.status.idle":"2023-12-18T22:01:15.251631Z","shell.execute_reply.started":"2023-12-18T22:01:15.196251Z","shell.execute_reply":"2023-12-18T22:01:15.250653Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[(Document(page_content=\"The function takes a spaCy Doc object (named doc) as a parameter and returns a Doc.\\nTherefore, we can use it as a another pipeline component and simply add it to the\\nexisting pipeline:\\nnlp.add_pipe (norm_entities )\\nNow we can repeat the process on the example sentences and check the result:\\ndoc = nlp(text)\\nprint(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\\\n')\\nOut:\\n(['Baker', 'International'], 'ORG')\\n(['New', 'York', 'Stock', 'Exchange'], 'ORG')\\nMerging Entity Tokens\", metadata={'source': '/kaggle/input/books-ds/blueprints-for-text-analytics-using-python-machine-learning-based-solutions-for-common-real-world-nlp-applications-149207408x-9781492074083_compress.pdf', 'page': 353}),\n  0.3947095102504933),\n (Document(page_content='if ref._.ref_n != \\'\\':\\n                        token._.ref_n = ref._.ref_n\\n                        token._.ref_t = ref._.ref_t\\n                        break\\n    return doc\\nAgain, we add this resolver to our pipeline and check the result:\\nnlp.add_pipe (anaphor_coref )\\ndoc = nlp(text)\\ndisplay_ner (doc).query(\"ref_n != \\'\\'\" ) \\\\\\n  [[\\'text\\', \\'ent_type\\' , \\'main_coref\\' , \\'ref_n\\', \\'ref_t\\']]\\nOut:\\ntext ent_type main_coref ref_n ref_t\\n0 Hughes Tool Co ORG Hughes Tool Co Hughes Tool Co ORG', metadata={'source': '/kaggle/input/books-ds/blueprints-for-text-analytics-using-python-machine-learning-based-solutions-for-common-real-world-nlp-applications-149207408x-9781492074083_compress.pdf', 'page': 361}),\n  0.3442400500046986),\n (Document(page_content='When processing larger datasets, it is recommended to use spaCy’s batch processing\\nfor a significant performance gain (roughly factor 2 on our dataset). The function\\nnlp.pipe  takes an iterable of texts, processes them internally as a batch, and yields a\\nlist of processed Doc objects in the same order as the input data.\\nTo use it, we first have to define a batch size. Then we can loop over the batches and\\ncall nlp.pipe .\\nbatch_size  = 50\\nfor i in range(0, len(df), batch_size ):', metadata={'source': '/kaggle/input/books-ds/blueprints-for-text-analytics-using-python-machine-learning-based-solutions-for-common-real-world-nlp-applications-149207408x-9781492074083_compress.pdf', 'page': 138}),\n  0.34134734320400184),\n (Document(page_content='In this chapter, we will develop blueprints for a text preprocessing pipeline. The pipe‐\\nline will take the raw text as input, clean it, transform it, and extract the basic features\\nof textual content. We start with regular expressions for data cleaning and tokeniza‐\\ntion and then focus on linguistic processing with spaCy . spaCy is a powerful NLP\\nlibrary with a modern API and state-of-the-art models. For some operations we will', metadata={'source': '/kaggle/input/books-ds/blueprints-for-text-analytics-using-python-machine-learning-based-solutions-for-common-real-world-nlp-applications-149207408x-9781492074083_compress.pdf', 'page': 110}),\n  0.33775938199026945)]"},"metadata":{}}]},{"cell_type":"code","source":"[doc.page_content.replace('\\n','') for doc in index.max_marginal_relevance_search(\"NLP Pipeline\",k=6)]","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:01:15.415492Z","iopub.execute_input":"2023-12-18T22:01:15.416311Z","iopub.status.idle":"2023-12-18T22:01:15.505859Z","shell.execute_reply.started":"2023-12-18T22:01:15.416278Z","shell.execute_reply":"2023-12-18T22:01:15.504975Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"[\"The function takes a spaCy Doc object (named doc) as a parameter and returns a Doc.Therefore, we can use it as a another pipeline component and simply add it to theexisting pipeline:nlp.add_pipe (norm_entities )Now we can repeat the process on the example sentences and check the result:doc = nlp(text)print(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\\\n')Out:(['Baker', 'International'], 'ORG')(['New', 'York', 'Stock', 'Exchange'], 'ORG')Merging Entity Tokens\",\n 'if ref._.ref_n != \\'\\':                        token._.ref_n = ref._.ref_n                        token._.ref_t = ref._.ref_t                        break    return docAgain, we add this resolver to our pipeline and check the result:nlp.add_pipe (anaphor_coref )doc = nlp(text)display_ner (doc).query(\"ref_n != \\'\\'\" ) \\\\  [[\\'text\\', \\'ent_type\\' , \\'main_coref\\' , \\'ref_n\\', \\'ref_t\\']]Out:text ent_type main_coref ref_n ref_t0 Hughes Tool Co ORG Hughes Tool Co Hughes Tool Co ORG',\n \"call nlp.pipe .batch_size  = 50for i in range(0, len(df), batch_size ):    docs = nlp.pipe(df['text'][i:i+batch_size ])    for j, doc in enumerate (docs):Feature Extraction on a Large Dataset | 117\",\n 'In this chapter, we will develop blueprints for a text preprocessing pipeline. The pipe‐line will take the raw text as input, clean it, transform it, and extract the basic featuresof textual content. We start with regular expressions for data cleaning and tokeniza‐tion and then focus on linguistic processing with spaCy . spaCy is a powerful NLPlibrary with a modern API and state-of-the-art models. For some operations we will',\n 'Before we start NLP processing, we initialize the new DataFrame  columns we want tofill with values:for col in nlp_columns :    df[col] = NonespaCy’s neural models benefit from running on GPU. Thus, we try to load the modelon the GPU before we start:if spacy.prefer_gpu ():    print(\"Working on GPU.\" )else:    print(\"No GPU found, working on CPU.\" )Now we have to decide which model and which of the pipeline components to use.',\n 'Based on these patterns, we create an EntityRuler  and add it to our pipeline:entity_ruler  = EntityRuler (nlp, patterns =patterns , overwrite_ents =True)nlp.add_pipe (entity_ruler )Now, when we call nlp, those organizations will automatically be labeled with the newtype GOV:text = \"\"\"Justice Department is an alias for the U.S. Department of Justice.Department of Transportation and the Securities and Exchange Commissionare government organisations, but the Sales Department is not.\"\"\"']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load LLM","metadata":{}},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.callbacks import StdOutCallbackHandler\nhandler = StdOutCallbackHandler()\nretriever = index.as_retriever()\nretriever.search_kwargs['fetch_k'] = 20\nretriever.search_kwargs['k'] = 10\nretriever.search_kwargs['max_marginal_relevance'] = True\nllm = ChatOpenAI(temperature=0.8)\nchain = RetrievalQA.from_chain_type(llm=llm,retriever=retriever,verbose=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:01:21.655825Z","iopub.execute_input":"2023-12-18T22:01:21.656510Z","iopub.status.idle":"2023-12-18T22:01:21.665262Z","shell.execute_reply.started":"2023-12-18T22:01:21.656472Z","shell.execute_reply":"2023-12-18T22:01:21.664296Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"chain.run('NLP Pipeline' ,\n         callbacks=[handler])","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:07:24.224707Z","iopub.execute_input":"2023-12-18T22:07:24.225124Z","iopub.status.idle":"2023-12-18T22:07:26.624479Z","shell.execute_reply.started":"2023-12-18T22:07:24.225093Z","shell.execute_reply":"2023-12-18T22:07:26.623510Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\n\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\nThe function takes a spaCy Doc object (named doc) as a parameter and returns a Doc.\nTherefore, we can use it as a another pipeline component and simply add it to the\nexisting pipeline:\nnlp.add_pipe (norm_entities )\nNow we can repeat the process on the example sentences and check the result:\ndoc = nlp(text)\nprint(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\\n')\nOut:\n(['Baker', 'International'], 'ORG')\n(['New', 'York', 'Stock', 'Exchange'], 'ORG')\nMerging Entity Tokens\n\nif ref._.ref_n != '':\n                        token._.ref_n = ref._.ref_n\n                        token._.ref_t = ref._.ref_t\n                        break\n    return doc\nAgain, we add this resolver to our pipeline and check the result:\nnlp.add_pipe (anaphor_coref )\ndoc = nlp(text)\ndisplay_ner (doc).query(\"ref_n != ''\" ) \\\n  [['text', 'ent_type' , 'main_coref' , 'ref_n', 'ref_t']]\nOut:\ntext ent_type main_coref ref_n ref_t\n0 Hughes Tool Co ORG Hughes Tool Co Hughes Tool Co ORG\n\nWhen processing larger datasets, it is recommended to use spaCy’s batch processing\nfor a significant performance gain (roughly factor 2 on our dataset). The function\nnlp.pipe  takes an iterable of texts, processes them internally as a batch, and yields a\nlist of processed Doc objects in the same order as the input data.\nTo use it, we first have to define a batch size. Then we can loop over the batches and\ncall nlp.pipe .\nbatch_size  = 50\nfor i in range(0, len(df), batch_size ):\n\nIn this chapter, we will develop blueprints for a text preprocessing pipeline. The pipe‐\nline will take the raw text as input, clean it, transform it, and extract the basic features\nof textual content. We start with regular expressions for data cleaning and tokeniza‐\ntion and then focus on linguistic processing with spaCy . spaCy is a powerful NLP\nlibrary with a modern API and state-of-the-art models. For some operations we will\n\nand print the components of the NLP pipeline:4\nnlp = spacy.load('en_core_web_sm' )\nprint(*nlp.pipeline , sep='\\n')\nOut:\n('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f98ac6443a0>)\n('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f98ac7a07c0>)\n('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f98ac7a0760>)\nOnce the text is processed, we can access the named entities directly with doc.ents .\n\nnlp.make_doc  on a text.\nProcessing Text\nThe pipeline is executed by calling the nlp object. The call returns an object of type\nspacy.tokens.doc.Doc , a container to access the tokens, spans (ranges of tokens),\nand their linguistic annotations.\nnlp = spacy.load(\"en_core_web_sm\" )\ntext = \"My best friend Ryan Peters likes fancy adventure games.\"\ndoc = nlp(text)\nspaCy is object-oriented as well as nondestructive. The original text is always\n\nThis Language  object now contains the shared vocabulary, the model, and the pro‐\ncessing pipeline. Y ou can check the pipeline components via this property of the\nobject:\nnlp.pipeline\nOut:\n[('tagger', <spacy.pipeline.pipes.Tagger at 0x7fbd766f84c0>),\n ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7fbd813184c0>),\n ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7fbd81318400>)]\nThe default pipeline consists of a tagger, parser, and named-entity recognizer ( ner),\n\nnlp = spacy.load('en_core_web_lg' )\npipes = [entity_ruler , norm_entities , merge_entities ,\n         init_coref , alias_resolver , name_resolver ,\n         neural_coref , anaphor_coref , norm_names ]\nfor pipe in pipes:\n    nlp.add_pipe (pipe)\nBefore we start the information extraction process, we create two additional rules for\nthe “executive-of ” relation similar to the “subsidiary-of ” relation and add them to our \nrule-based matcher :\n\nfrom spacy.pipeline  import merge_entities\nnlp.add_pipe (merge_entities )\ndoc = nlp(text)\nprint(*[(t.text, t.ent_type_ ) for t in doc if t.ent_type_  != ''])\n332 | Chapter 12: Building a Knowledge Graph\n\nof, ” and “executive-of. ” Figure 12-6  shows the resulting graph with some selected\nsubgraphs.\nTo get the best results in dependency parsing and named-entity recognition, we use \nspaCy’s large model with our complete pipeline. If possible, we will use the GPU to\nspeed up NLP processing:\nif spacy.prefer_gpu ():\n    print(\"Working on GPU.\" )\nelse:\n    print(\"No GPU found, working on CPU.\" )\nnlp = spacy.load('en_core_web_lg' )\npipes = [entity_ruler , norm_entities , merge_entities ,\nHuman: NLP Pipeline\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'The NLP pipeline is a series of steps or components that are applied to text in order to process and analyze it. These components can include tasks such as tokenization, part-of-speech tagging, dependency parsing, named entity recognition, and coreference resolution. The pipeline is typically defined using a NLP library, such as spaCy, and can be customized by adding or removing components based on the specific needs of the task or application.'"},"metadata":{}}]},{"cell_type":"markdown","source":"As we notice the previous answer was from the Parameteric knowledge not from the source knowledge ","metadata":{}},{"cell_type":"markdown","source":"# Mistral LLM\n","metadata":{}},{"cell_type":"code","source":"!pip install -q torch datasets\n!pip install -q accelerate==0.21.0 \\\n                peft==0.4.0 \\\n                bitsandbytes==0.40.2 \\\n                transformers==4.34.0 \\\n                trl==0.4.7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- You have to restart the kernel after the above installation ","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else 'cpu'\nmodel_name='/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:43:30.170577Z","iopub.execute_input":"2023-12-18T21:43:30.170976Z","iopub.status.idle":"2023-12-18T21:43:32.555563Z","shell.execute_reply.started":"2023-12-18T21:43:30.170943Z","shell.execute_reply":"2023-12-18T21:43:32.554758Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!ls $model_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:43:37.273814Z","iopub.execute_input":"2023-12-18T21:43:37.274313Z","iopub.status.idle":"2023-12-18T21:43:37.368834Z","shell.execute_reply.started":"2023-12-18T21:43:37.274282Z","shell.execute_reply":"2023-12-18T21:43:37.367841Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_config = transformers.AutoConfig.from_pretrained(model_name, trust_remote_code=True)\nmodel_config","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:43:53.748674Z","iopub.execute_input":"2023-12-18T21:43:53.749045Z","iopub.status.idle":"2023-12-18T21:43:53.759596Z","shell.execute_reply.started":"2023-12-18T21:43:53.749017Z","shell.execute_reply":"2023-12-18T21:43:53.758740Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"MistralConfig {\n  \"_name_or_path\": \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.34.0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}"},"metadata":{}}]},{"cell_type":"code","source":"\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:47:40.250577Z","iopub.execute_input":"2023-12-18T21:47:40.250994Z","iopub.status.idle":"2023-12-18T21:47:40.257550Z","shell.execute_reply.started":"2023-12-18T21:47:40.250957Z","shell.execute_reply":"2023-12-18T21:47:40.256688Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from timeit import default_timer\nt0 = default_timer()\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name,\n    config=model_config,\n    quantization_config=bnb_config,\n    torch_dtype = torch.bfloat16,\n    device_map=\"auto\"\n)\nprint(default_timer() - t0, 'sec.')","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:48:47.809694Z","iopub.execute_input":"2023-12-18T21:48:47.810102Z","iopub.status.idle":"2023-12-18T21:51:22.813120Z","shell.execute_reply.started":"2023-12-18T21:48:47.810070Z","shell.execute_reply":"2023-12-18T21:51:22.812148Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b950a9df5f024d29a8b4cbb41aae6a4c"}},"metadata":{}},{"name":"stdout","text":"154.99799609000047 sec.\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain.chains import LLMChain\ntext_generation_pipeline = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    temperature=0.8,\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=300,\n    pad_token_id = tokenizer.eos_token_id\n)\n\nmistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:06:56.280151Z","iopub.execute_input":"2023-12-18T22:06:56.280892Z","iopub.status.idle":"2023-12-18T22:06:56.287054Z","shell.execute_reply.started":"2023-12-18T22:06:56.280858Z","shell.execute_reply":"2023-12-18T22:06:56.286048Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"qa = RetrievalQA.from_chain_type(\n    llm=mistral_llm, \n    retriever=retriever, \n    verbose=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:06:59.945080Z","iopub.execute_input":"2023-12-18T22:06:59.945791Z","iopub.status.idle":"2023-12-18T22:06:59.950797Z","shell.execute_reply.started":"2023-12-18T22:06:59.945748Z","shell.execute_reply":"2023-12-18T22:06:59.949689Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"qa.run('What is text summarization algorthms',callbacks=[handler])","metadata":{"execution":{"iopub.status.busy":"2023-12-18T22:07:01.564471Z","iopub.execute_input":"2023-12-18T22:07:01.565415Z","iopub.status.idle":"2023-12-18T22:07:11.263450Z","shell.execute_reply.started":"2023-12-18T22:07:01.565370Z","shell.execute_reply":"2023-12-18T22:07:11.262584Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n\n\n\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nvate others. Text summarization  is defined as the method used for generating a con‐\ncise summary of longer text while still conveying useful information and without\nlosing the overall context. This is a method that we are quite familiar with: when\n243\n\nright approach for any application.\nText Summarization\nIt is likely that you have undertaken a summarization task knowingly or unknowingly\nat some point in life. Examples are telling a friend about a movie you watched last\nnight and trying to explain your work to your family. We all like to provide a brief\nsummary of our experiences to the rest of the world to share our feelings and moti‐\nvate others. Text summarization  is defined as the method used for generating a con‐\n\na few lines or a paragraph and make it digestible to most users. Applications of text\nsummarization can be found not just on the internet but also in fields like paralegal\ncase summaries, book synopses, etc.\nWhat You’ll Learn and What We’ll Build\nIn this chapter, we will start with an introduction to text summarization and provide\nan overview of the methods used. We will analyze different types of text data and\n\nCHAPTER 9\nText Summarization\nThere is a massive amount of information on the internet on every topic. A Google\nsearch returns millions of search results containing text, images, videos, and so on.\nEven if we consider only the text content, it’s not possible to read through it all. Text \nsummarization methods are able to condense text information to a short summary of\na few lines or a paragraph and make it digestible to most users. Applications of text\n\nreading course textbooks, lecture notes, or even this book, many students will try to\nhighlight important sentences or make short notes to capture the important concepts.\nAutomatic text summarization methods allow us to use computers to do this task.\nSummarization methods can be broadly classified into extraction  and abstraction\nmethods. In extractive summarization, important phrases or sentences are identified\nin a given body of text and combined to form the summary of the entire text. Such\n\ngives us the ability to quickly test different summarization methods and change the\ndefault configurations to suit specific use cases. For now, we will go with the default\noptions, including identifying the top three sentences:1\nfrom sumy.parsers.plaintext  import PlaintextParser\nfrom sumy.nlp.tokenizers  import Tokenizer\nfrom sumy.nlp.stemmers  import Stemmer\nfrom sumy.utils  import get_stop_words\nfrom sumy.summarizers.lsa  import LsaSummarizer\nLANGUAGE  = \"english\"\n\nan overview of the methods used. We will analyze different types of text data and\ntheir specific characteristics that are useful in determining the choice of summariza‐\ntion method. We will provide blueprints that apply these methods to different use\ncases and analyze their performance. At the end of this chapter, you will have a good\nunderstanding of different text summarization methods and be able to choose the\nright approach for any application.\nText Summarization\n\nciently. We can consider this an example of text summarization applied to long-form\ntext. Another use case might be a media company that wants to send a newsletter to\nits subscribers every morning highlighting the important events of the previous day.\nCustomers don’t appreciate long emails, and therefore creating a short summary of\neach article is important to keep them engaged. In this use case, you need to summa‐\n\nLet’s say you are working with a legal firm that wants to review historical cases to help\nprepare for a current case. Since case proceedings and judgments are very long, they\nwant to generate summaries and review the entire case only if it’s relevant. Such a\nsummary helps them to quickly look at multiple cases and allocate their time effi‐\nciently. We can consider this an example of text summarization applied to long-form\n\ntures, and then train the model.\nClosing Remarks\nIn this chapter, we introduced the concept of text summarization and provided blue‐\nprints that can be used to generate summaries for different use cases. If you are look‐\ning to generate summaries from short text such as web pages, blogs, and news\narticles, then the first blueprint based on topic representation using the LSA summa‐\nrizer would be a good choice. If you are working with much larger text such as\n\nQuestion: What is text summarization algorthms\nHelpful Answer:\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n\u001b[1m> Finished chain.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"' Text summarization algorithms are computational methods used to automatically generate a concise summary of longer text while still conveying useful information and without losing the overall context.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}